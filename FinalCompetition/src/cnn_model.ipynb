{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import Library"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport math\nimport time\nimport pickle\nimport random\n\nimport librosa\nfrom scipy.io import wavfile\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fastprogress import master_bar, progress_bar\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import transforms","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Const Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"duration = 3\nduration_long = 5\nSAMPLE_RATE = 44100\nSAMPLE_DATA_CNT = SAMPLE_RATE*duration # 220500\nSAMPLE_DATA_CNT_LONG = SAMPLE_RATE*duration_long\n\n## spectrogram\nn_mels = 128\nfmin = 20\nfmax = SAMPLE_RATE // 2\nn_mels = 128\nn_fft = n_mels * 20","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Path"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '../input/freesound-audio-tagging-2019/'\n\nCURATED_CSV = DATA_PATH + 'train_curated.csv'\nCURATED_DIR = DATA_PATH + 'train_curated/'\nNOISY_CSV = DATA_PATH + 'train_noisy.csv'\nNOISY_DIR = DATA_PATH + 'train_noisy/'\nTEST_CSV = DATA_PATH + 'sample_submission.csv'\nTEST_DIR = DATA_PATH + 'test/'\n\n\nSPEC_DATA_PATH = \"../input/preprocessed-data-spectrogram/\"\n\nBEST_50_CSV = SPEC_DATA_PATH + \"trn_noisy_best50s.csv\"\nCURATED_TRAIN_MEL = SPEC_DATA_PATH + \"mels_train_curated.pkl\"\nNOISY_BEST50_TRAIN_MEL = SPEC_DATA_PATH + \"mels_trn_noisy_best50s.pkl\"\nCURATED_TRAIN_MFCC = SPEC_DATA_PATH + \"mfcc_train_curated.pkl\"\nNOISY_BEST50_TRAIN_MFCC = SPEC_DATA_PATH + \"mfcc_trn_noisy_best50s.pkl\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load preprocessed spectrogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_pkl(filename):\n    with open(filename, 'rb') as f:\n        return pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# mels\nx_train_curated_mels = load_pkl(CURATED_TRAIN_MEL)\nx_train_noisy_mels = load_pkl(NOISY_BEST50_TRAIN_MEL)\n\n# mfcc\nx_train_curated_mfcc = load_pkl(CURATED_TRAIN_MFCC)\nx_train_noisy_mfcc = load_pkl(NOISY_BEST50_TRAIN_MFCC)\n\ntrain_curated_csv = pd.read_csv(CURATED_CSV)\ntrain_noisy_csv = pd.read_csv(BEST_50_CSV)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drop corrupted / wrong label file"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndrop_audio_file = ['f76181c4.wav', '77b925c2.wav', '6a1f682a.wav', 'c7db12aa.wav', '7752cc8a.wav','1d44b0bd.wav']\ndrop_index = train_curated_csv.loc[  train_curated_csv['fname'].isin( drop_audio_file ) ].index.values\ntrain_curated_csv = train_curated_csv.drop(drop_index, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Concatenate two lists"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nx_train_mels = x_train_curated_mels\nx_train_mels.extend(x_train_noisy_mels)\ndel x_train_curated_mels, x_train_noisy_mels\n\nx_train_mfcc = x_train_curated_mfcc\nx_train_mfcc.extend(x_train_noisy_mfcc)\ndel x_train_curated_mfcc, x_train_noisy_mfcc\n\ntrain_csv = pd.concat([train_curated_csv, train_noisy_csv], ignore_index=True)\ny_train = train_csv.labels.str.get_dummies(sep=',')\ndel train_csv, train_curated_csv, train_noisy_csv\n\nprint(len(x_train_mels), len(x_train_mfcc), len(y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocess Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"## OPEN AND TRIM THE AUDIO FILE\ndef read_audio(pathdir, trim_long_data):\n    rate, data = wavfile.read(pathdir)\n    data = data.astype(float)\n\n    # workaround: 0 length causes error\n    if 0 < len(data): \n        data, _ = librosa.effects.trim(data)  # trim, top_db=default(60)\n\n    # make it unified length to SAMPLE_DATA_CNT\n    if len(data) > SAMPLE_DATA_CNT_LONG:             \n        if trim_long_data:\n#             print(pathdir)\n            # trim the audio where is nearby the maximun amplitude to a segment of 7 secs\n            MaxAmp = np.where(abs(data) == max(abs(data)))[0]\n            MaxAmp = MaxAmp[np.random.randint(len(MaxAmp))]\n            max_offset = len(data) - SAMPLE_DATA_CNT_LONG\n            if max(abs(data)) == 0:\n                data = data[:SAMPLE_DATA_CNT_LONG]\n            elif (len(data)-MaxAmp) < MaxAmp:\n                offset = (np.random.randint(len(data)-MaxAmp) if len(data)-MaxAmp < max_offset else len(data)-MaxAmp-np.random.randint(max_offset))\n                data = data[(MaxAmp-(SAMPLE_DATA_CNT_LONG-offset)):(MaxAmp+offset)]\n            else:\n                if MaxAmp == 0:\n                    offset = 0\n                elif MaxAmp < max_offset:\n                    offset = np.random.randint(MaxAmp)\n                else:\n                    offset = MaxAmp-np.random.randint(max_offset)\n                data = data[(MaxAmp-offset):(MaxAmp+(SAMPLE_DATA_CNT_LONG-offset))]\n    else:                                     \n        data = np.tile(data, math.floor(SAMPLE_DATA_CNT/data.shape[0])) \n        if len(data) < SAMPLE_DATA_CNT:\n            max_offset = SAMPLE_DATA_CNT-len(data)\n            offset = np.random.randint(max_offset)\n            data = np.pad(data, (offset, SAMPLE_DATA_CNT-len(data)-offset), \"constant\")\n\n    return data\n\ndef audio_to_melspectrogram(audio):\n    spectrogram = librosa.feature.melspectrogram(audio,sr=SAMPLE_RATE,n_mels=n_mels,n_fft=n_fft,fmin=fmin,fmax=fmax)\n    spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n    mfcc = librosa.feature.mfcc(S=spectrogram, n_mfcc=13)\n    mfcc = librosa.feature.delta(mfcc, order=2)\n    return spectrogram, mfcc\n\ndef read_as_melspectrogram(pathdir, trim_long_data):\n    x = read_audio(pathdir, trim_long_data)\n    mels, mfcc = audio_to_melspectrogram(x)\n    return mels, mfcc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change the array to RBG scaler\ndef mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n    # Stack X as [X,X,X]\n    X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    X = X - mean\n    std = std or X.std()\n    Xstd = X / (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    norm_max = norm_max or _max\n    norm_min = norm_min or _min\n    if (_max - _min) > eps:\n        # Normalize to [0, 255]\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255 * (V - norm_min) / (norm_max - norm_min)\n        V = V.astype(np.uint8)\n    else:\n        # Just zero\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_wav_to_image(csv_path):\n    X_mels = []\n    X_mfcc = []\n    test_data = pd.read_csv(csv_path)\n    for filename in test_data.fname:\n        x_mels, x_mfcc = read_as_melspectrogram(TEST_DIR+filename , trim_long_data=True)\n        x_mels_color = mono_to_color(x_mels)\n        X_mels.append(x_mels_color)\n        X_mfcc.append(x_mfcc)\n    return X_mels, X_mfcc\n\ndef save_as_pkl_binary(obj, filename):\n    with open(filename, 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Whether CUDA is available"},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NN model define"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"## convolution block\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n        self._init_weights()\n        \n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.zeros_(m.bias)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.avg_pool2d(x, 2)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Classifier\nclass Classifier(nn.Module):\n    def __init__(self, num_classes, n_dim):\n        super().__init__()\n        \n        self.conv = nn.Sequential(\n            ConvBlock(in_channels=n_dim, out_channels=32),\n            ConvBlock(in_channels=32, out_channels=64),\n            ConvBlock(in_channels=64, out_channels=128),\n            ConvBlock(in_channels=128, out_channels=256),\n            ConvBlock(in_channels=256, out_channels=512),\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(512, 128),\n            nn.PReLU(),\n            nn.BatchNorm1d(128),\n            nn.Dropout(0.1),\n            nn.Linear(128, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = torch.mean(x, dim=3)\n        x, _ = torch.max(x, dim=2)\n        x = self.fc(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Show the model structure\nClassifier(80,3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## lwlrap Function\n [reference](https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8?fbclid=IwAR0CDfIoN_LJyTFEmW50-wRLoqRJIsdh79oSV2xpir_3ajyJbzwoV2xK21U#scrollTo=5HfziEYbodWk)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _one_sample_positive_class_precisions(scores, truth):\n  \"\"\"Calculate precisions for each true class for a single sample.\n  \n  Args:\n    scores: np.array of (num_classes,) giving the individual classifier scores.\n    truth: np.array of (num_classes,) bools indicating which classes are true.\n\n  Returns:\n    pos_class_indices: np.array of indices of the true classes for this sample.\n    pos_class_precisions: np.array of precisions corresponding to each of those\n      classes.\n  \"\"\"\n  num_classes = scores.shape[0]\n  pos_class_indices = np.flatnonzero(truth > 0)\n  # Only calculate precisions if there are some true classes.\n  if not len(pos_class_indices):\n    return pos_class_indices, np.zeros(0)\n  # Retrieval list of classes for this sample. \n  retrieved_classes = np.argsort(scores)[::-1]\n  # class_rankings[top_scoring_class_index] == 0 etc.\n  class_rankings = np.zeros(num_classes, dtype=np.int)\n  class_rankings[retrieved_classes] = range(num_classes)\n  # Which of these is a true label?\n  retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n  retrieved_class_true[class_rankings[pos_class_indices]] = True\n  # Num hits for every truncated retrieval list.\n  retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n  # Precision of retrieval list truncated at each hit, in order of pos_labels.\n  precision_at_hits = (\n      retrieved_cumulative_hits[class_rankings[pos_class_indices]] / \n      (1 + class_rankings[pos_class_indices].astype(np.float)))\n  return pos_class_indices, precision_at_hits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_per_class_lwlrap(truth, scores):\n  \"\"\"Calculate label-weighted label-ranking average precision.\n  \n  Arguments:\n    truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n      of presence of that class in that sample.\n    scores: np.array of (num_samples, num_classes) giving the classifier-under-\n      test's real-valued score for each class for each sample.\n  \n  Returns:\n    per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each \n      class.\n    weight_per_class: np.array of (num_classes,) giving the prior of each \n      class within the truth labels.  Then the overall unbalanced lwlrap is \n      simply np.sum(per_class_lwlrap * weight_per_class)\n  \"\"\"\n  assert truth.shape == scores.shape\n  num_samples, num_classes = scores.shape\n  # Space to store a distinct precision value for each class on each sample.\n  # Only the classes that are true for each sample will be filled in.\n  precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n  for sample_num in range(num_samples):\n    pos_class_indices, precision_at_hits = (\n      _one_sample_positive_class_precisions(scores[sample_num, :], \n                                            truth[sample_num, :]))\n    precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n        precision_at_hits)\n  labels_per_class = np.sum(truth > 0, axis=0)\n  weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n  # Form average of each column, i.e. all the precisions assigned to labels in\n  # a particular class.\n  per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) / \n                      np.maximum(1, labels_per_class))\n  # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n  #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n  #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n  #                = np.sum(per_class_lwlrap * weight_per_class)\n  return per_class_lwlrap, weight_per_class","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DataSet"},{"metadata":{},"cell_type":"markdown","source":"問題：<br>\n15秒的音訊檔轉出來的spectrogram大小為(128,1292)<br>\n而該kernel只能承受(128,128)的圖片大小<br>\n如果使用resize是否會造成圖片太過壓縮?<br><br>\n\n解決方式1: img.resize((width, height),Image.ANTIALIAS) //平滑處理 <br>\n解決方式2: 先切割出一段區間的音訊檔，再使用resize"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FATTrainDataset(Dataset):\n    def __init__(self, mels, labels, transforms):\n        super().__init__()\n        self.mels = mels\n        self.labels = labels\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.mels)\n    \n    def __getitem__(self, idx):\n        # crop 1sec\n        image = Image.fromarray(self.mels[idx])\n        time_dim, base_dim = image.size\n        crop = random.randint(0, time_dim - 256)\n        image = image.crop([crop, 0, crop + 256, base_dim])\n#         image = image.crop([0, 0, 256, base_dim])\n        \n        image = self.transforms(image).div_(255)\n        \n        label = self.labels[idx]\n        label = torch.from_numpy(label).float()\n        \n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FATTestDataset(Dataset):\n    def __init__(self, fnames, mels, transforms, tta=5):\n        super().__init__()\n        self.fnames = fnames\n        self.mels = mels\n        self.transforms = transforms\n        self.tta = tta\n        \n    def __len__(self):\n        return len(self.fnames) * self.tta\n    \n    def __getitem__(self, idx):\n        new_idx = idx % len(self.fnames)\n        \n        image = Image.fromarray(self.mels[new_idx])\n        time_dim, base_dim = image.size\n        crop = random.randint(0, time_dim - 256)\n        image = image.crop([crop, 0, crop + 256, base_dim])\n#         image = image.crop([0, 0, 256, base_dim])\n        \n        image = self.transforms(image).div_(255)\n        fname = self.fnames[new_idx]\n        \n        return image, fname","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = transforms.Compose([transforms.RandomHorizontalFlip(0.5),transforms.ToTensor()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train class"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(x_train, y_train, train_transforms, dataset='mels'):\n    model_name = dataset + '_weight_best.pt'\n    num_epochs = 80\n    batch_size = 64\n    test_batch_size = 256\n    lr = 3e-3\n    eta_min = 1e-5\n    t_max = 10\n    \n    num_classes = y_train.shape[1]\n    n_dim = 3 if dataset == 'mels' else 1\n    \n    x_trn, x_val, y_trn, y_val = train_test_split(x_train, y_train.values, test_size=0.2)\n    \n    train_dataset = FATTrainDataset(x_trn, y_trn, train_transforms)\n    valid_dataset = FATTrainDataset(x_val, y_val, train_transforms)\n\n    '''\n    用於批次訓練\n    '''\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=test_batch_size, shuffle=False)\n\n    model = Classifier(num_classes=num_classes, n_dim=n_dim).cuda()\n    criterion = nn.BCEWithLogitsLoss().cuda()\n    optimizer = Adam(params=model.parameters(), lr=lr, amsgrad=False)\n    scheduler = CosineAnnealingLR(optimizer, T_max=t_max, eta_min=eta_min)\n\n    best_epoch = -1\n    best_lwlrap = 0.\n    mb = master_bar(range(num_epochs))\n\n    for epoch in mb:\n    \n        start_time = time.time()\n        \n        '''\n        訓練模型\n        '''\n        model.train()\n        avg_loss = 0.        \n        for x_batch, y_batch in progress_bar(train_loader, parent=mb):\n            \n            preds = model(x_batch.cuda())               # Forward propagation\n            loss = criterion(preds, y_batch.cuda())     # 計算loss\n\n            optimizer.zero_grad()                       # 梯度歸零\n            loss.backward()                             # Back propagation\n            optimizer.step()                            # update 參數\n\n            avg_loss += loss.item() / len(train_loader)     \n        model.eval()\n        \n        '''\n        測試模型 用來看這次的訓練成效\n        '''\n        valid_preds = np.zeros((len(x_val), num_classes))\n        avg_val_loss = 0.\n        for i, (x_batch, y_batch) in enumerate(valid_loader):\n            preds = model(x_batch.cuda()).detach()\n            loss = criterion(preds, y_batch.cuda())\n\n            preds = torch.sigmoid(preds)\n            valid_preds[i * test_batch_size: (i+1) * test_batch_size] = preds.cpu().numpy()\n\n            avg_val_loss += loss.item() / len(valid_loader)\n     \n        score, weight = calculate_per_class_lwlrap(y_val, valid_preds)\n        lwlrap = (score * weight).sum()\n        ### 測試模型尾巴\n        \n        scheduler.step()\n        \n        '''\n        單純打印出來\n        '''\n        if (epoch + 1) % 5 == 0:\n            elapsed = time.time() - start_time\n            mb.write(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  val_lwlrap: {lwlrap:.6f}  time: {elapsed:.0f}s')\n    \n        if lwlrap > best_lwlrap:\n            best_epoch = epoch + 1\n            best_lwlrap = lwlrap\n            torch.save(model.state_dict(), model_name)    # 只保存網路中的参数 (速度快, 占内存少)\n      \n      \n    return {\n        'best_epoch': best_epoch,\n        'best_lwlrap': best_lwlrap,\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_mels[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nresult_mels = train_model(x_train_mels, y_train, tr, 'mels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nresult_mfcc = train_model(x_train_mfcc, y_train, tr, 'mfcc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict class"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nx_test_mels, x_test_mfcc = convert_wav_to_image(TEST_CSV)\nsave_as_pkl_binary(x_test_mels, 'x_test_mels.pkl')\nsave_as_pkl_binary(x_test_mfcc, 'x_test_mfcc.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = pd.read_csv(TEST_CSV)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_model(test_fnames, x_test, test_transforms, num_classes, *, tta=5, dataset='mels'):\n    model_name = dataset + '_weight_best.pt'\n    n_dim = 3 if dataset == 'mels' else 1\n    batch_size = 256\n\n    test_dataset = FATTestDataset(test_fnames.values, x_test, test_transforms, tta=tta)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    model = Classifier(num_classes=num_classes, n_dim=n_dim)\n    model.load_state_dict(torch.load(model_name))\n    model.cuda()\n    model.eval()\n\n    all_outputs, all_fnames = [], []\n\n    pb = progress_bar(test_loader)\n    for images, fnames in pb:\n        preds = torch.sigmoid(model(images.cuda()).detach())\n        all_outputs.append(preds.cpu().numpy())\n        all_fnames.extend(fnames)\n\n    test_preds = pd.DataFrame(data=np.concatenate(all_outputs),\n                              index=all_fnames,\n                              columns=map(str, range(num_classes)))\n                              \n    ## 因為使用dataload 同一個filename 可能會有兩個predict結果\n    ## 所以要用groupby(level=0)組合起來再取平均\n    test_preds = test_preds.groupby(level=0).mean() \n\n    return test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_preds_mels = predict_model(y_test.fname, x_test_mels, tr, 80, tta=20, dataset='mels')\ntest_preds_mfcc = predict_model(y_test.fname, x_test_mfcc, tr, 80, tta=20, dataset='mfcc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = y_test.columns[1:].tolist()\ny_test[labels] = test_preds_mels.values*0.8+test_preds_mfcc.values*0.2\ny_test.to_csv('submission.csv', index=False)\ny_test.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}